{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585c7934",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "What are our learning objectives for this lesson?\n",
    "\n",
    "* Learn about the Perceptron achitecture\n",
    "* Learn how to train a Perceptron model \n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Marsland, Stephen. Machine Learning: An Algorithmic Perspective 2nd ed. (2015).\n",
    "\n",
    "## Lab Tasks\n",
    "\n",
    "1. Initialize weight vector $\\vec{w}$ with random values\n",
    "2. Implement an activation function\n",
    "3. Implement an update function\n",
    "\n",
    "### Initialize a weight vector\n",
    "\n",
    "#### What is the dimensionality of this vector?\n",
    "\n",
    "Let's first think through the dimensionality of our weight vector. How many entries should it have? Recall that the sum $h$ is given by\n",
    "\n",
    "$$h = \\sum_{i=1}^{m}w_i x_i $$\n",
    "\n",
    "where $w_i$ is the $i^{th}$ entry of the weight vector $\\vec{w}$ and $x_i$ is the $i^{th}$ entry of the input vector $\\vec{x}$. This means that for every input value we will need a weight value that goes with it. In another word, if there are $n$ entries in our input vector $\\vec{x}$, we will need a weight vector $\\vec{w}$ of length $n$. \n",
    "\n",
    "Recall that we want to include an additional bias node in our model where the input is always a non-zero value. Since we will be alterning the weight $w_b$ associated with the bias node at the same time as all other nodes in our network, it might be convinient to dedicate a spot in our weight vector $\\vec{w}$ that represents the weight $w_0$. Thus, for a problem with $n$ inputs, $\\vec{w}$ might look something like this:\n",
    "\n",
    "$$\\vec{w}^T = [w_0, w_1, w_2, ..., w_n]$$\n",
    "\n",
    "Notice that $\\vec{w}$ has a length of $n+1$. \n",
    "\n",
    "\n",
    "#### What initial value do we assign the vector?\n",
    "\n",
    "The initial value we assign the entries in our weight vector $\\vec{w}$ matters. If we assign very large values, then $h$ will also be a very large value regardless of our input values (given that input values are not all zeros). If we assign extremely small values, we run into the opposite problem, where $h$ will also be a extremely small value regardless of the input values. We also want to avoid initializing it with all zeros. If you're interested in understanding why, please take a look at this [stackexcahnge response](https://datascience.stackexchange.com/questions/26134/initialize-perceptron-weights-with-zero) that gives a concise explanation. \n",
    "\n",
    "In conclusion, it is ideal to have weights that are neither too big or too small. For neural networks (which we will get to soon), The last desired quaility in our initialization values would be non-uniformity. We don't want to assign a single value across the board, and the way to achieve this is by using pseudorandom generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# define inputs\n",
    "inputs = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "targets = np.array([[0],[1],[1],[1]])\n",
    "\n",
    "# capture how many nodes are needed by examining\n",
    "# how many entries is in a set of input\n",
    "n_in = np.shape(inputs)[1]\n",
    "        \n",
    "# how many values are we expecting as output?\n",
    "# in this case, we want a single value as our output\n",
    "n_out = 1\n",
    "        \n",
    "# η is the rate of which we learn from our data\n",
    "# typically, 0.1 < η < 0.4\n",
    "eta = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: insert a bias value of 1 into the inputs\n",
    "\n",
    "def insert_bias(_inputs):\n",
    "    # complete this function\n",
    "\n",
    "inputs = insert_bias(inputs)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a142bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: initialize a weight vector\n",
    "\n",
    "# how long does it have to be? \n",
    "# Don't forget the bias node that we are including\n",
    "# what initial values would you assign it?\n",
    "\n",
    "# complete the initialization below\n",
    "\n",
    "weights = None\n",
    "\n",
    "print(f'weight vector:\\n{weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d977e23",
   "metadata": {},
   "source": [
    "### Implement an activation function\n",
    "\n",
    "The activation of each neuron $j$ using activation function $g$ is given by:\n",
    "\n",
    "$$y_j = g(\\sum_{i=0}^m w_{ij}x_i)=\\begin{cases}1 \\quad \\text{  if  }g(\\sum_{i=0}^m w_{ij}x_i)>0 \\\\ 0 \\quad \\text{  if  } g(\\sum_{i=0}^m w_{ij}x_i) \\leq 0 \\end{cases}$$\n",
    "\n",
    "Implement an activation function according to the above equation. Here are several things to consider:\n",
    "* What parameters do you need to pass to this function?\n",
    "* How could you use numpy to compute $\\vec{y}$ instead of writing a for loop to compute each $y_j$ individually? \n",
    "* What kind of object is this function going to return?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfefb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the y_j and save it to an activation array\n",
    "\n",
    "def calculate_activations(_inputs, _weights):\n",
    "    \n",
    "    # complete this function\n",
    "    \n",
    "    return activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241ed8b",
   "metadata": {},
   "source": [
    "### Update the weight vector\n",
    "\n",
    "Let $y_j$ be the output of the $j^{th}$ neuron, and $t_j$ the target for that neuron, and $\\eta$ (eta) the learning rate. \n",
    "\n",
    "$$w_{ij} \\leftarrow w_{ij} - \\eta(y_j - t_j) \\cdot x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: adjust the weights based on the error between activation and target\n",
    "\n",
    "def update_weights(_inputs, _targets, _activations, _weights):\n",
    "    \n",
    "    # complete this funtion\n",
    "    \n",
    "    return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0ee8c",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now that you have imeplemented the ```calculate_activations``` function and ```update_weights``` function, we can start training the Perceptron model. There are 3 steps in training:\n",
    "1. calculate the activation based on the current weights.\n",
    "2. update the weight vector based on the gradient of the error for this time step.\n",
    "3. repeat for $n$ iterations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 5\n",
    "\n",
    "# TODO: implements the steps to be taken per iteration\n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    # complete this loop\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ecaa53",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "\n",
    "Now we feed the trained model a test set to see how accurate the model is. Run the cell below to see the accuracy of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[1,0],[1,1],[0,1],[0,0]])\n",
    "test_target = np.array([[1],[1],[1],[0]])\n",
    "\n",
    "# insert bias into the test set\n",
    "test = insert_bias(test)\n",
    "\n",
    "# getting the activations array\n",
    "activations = calculate_activations(test, weights)\n",
    "\n",
    "# check the accuracy of your trained model\n",
    "accuracy = 0.0\n",
    "for i in range(len(test_target)):\n",
    "    if test_target[i] == activations[i]: accuracy += 1        \n",
    "print(f'accuracy: {(accuracy / len(test_target))*100.0}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbb4d1",
   "metadata": {},
   "source": [
    "### Bonus Task\n",
    "\n",
    "Train your model to learn the XOR operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586eac58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
