{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7825a6a0",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "What are our learning objectives for this lesson?\n",
    "\n",
    "* Learn how to apply support vector machine in a classification problem\n",
    "* Get familiar with the scikit-learn library \n",
    "\n",
    "In this lab, we will use the support vector machine to solve a classification problem. We will be examine an iris dataset where each flower in the dataset belongs to one of three specific species in the iris genus. Each flower is represetned by their features along with their classification. At the end of this lab, we should have a model that is able to reliably classify the irises as their respective species. \n",
    "\n",
    "Content used in this lesson is based upon information in the following sources:\n",
    "* Marsland, Stephen. Machine Learning: An Algorithmic Perspective 2nd ed. (2015).\n",
    "* Pedregosa et al., Scikit-learn: Machine Learning in Python, JMLR 12, pp.\n",
    "\n",
    "## Lab Tasks \n",
    "\n",
    "1. Explore the Iris Dataset from scikit-learn\n",
    "2. Split the data into a training set and a test set\n",
    "3. Set up a support vector machine using scikit-learn\n",
    "4. Train and Test the model\n",
    "\n",
    "### Explor the Iris Dataset\n",
    "\n",
    "The Iris data set was created by the British statistician and biologist Ronald Fisher. It was included in his 1936 paper *The Use of Multiple Measurements in Taxonomic Problems* as an example of linear discriminant analysis. This data sets is composed of 3 different types of irises: Setosa, Versicolour, and Virginica. \n",
    "\n",
    "Before Running the cell below, please make sure you have sklearn installed. Run ```pip install -U scikit-learn``` if you haven't installed it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fbbadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the Iris dataset from scikit-learn\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Examine the Iris dataset. You can do that by printing out its features, \n",
    "#       its targets, or print out a portion of the data to examine its ranges.\n",
    "#       \n",
    "#       Documentation for the Iris dataset on scikit-learn: \n",
    "#       https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37cd175",
   "metadata": {},
   "source": [
    "#### How many features are used to represent each flower? What are they? \n",
    "\n",
    "Now that we have a rough understanding of the structure of the dataset, let's plot some of the data points to visualize them using ```matplotlib```. There might be more features in the dataset than the maximum dimension supported by the library. In that case, reduce the dimension of the dataset so that that the number of features is the same as the number of dimensions for your visualization by one of the two methods:\n",
    "1. By droping some features\n",
    "2. Projecting your higher-dimensional data to a lower-dimensional space\n",
    "\n",
    "To project the higher-dimensional data to a lower-dimensional space, we can use Singular Value Decomposition (SVD). This method of dimensionality reduction is availble in ```sklearn```. If you choose the second route to avoid loosing information from the dataset, you can add this import statement to perform SVD: ```from sklearn.decomposition import PCA```\n",
    "\n",
    "#### üíÅ‚Äç‚ôÄÔ∏è You are encourged to try both methods listed above. \n",
    "\n",
    "### Creating a 2D plot with Matplotlib\n",
    "\n",
    "You could practice plotting in a 2D space first if you are new to plotting with Python. We will go through how to use matplotlib to visualize 2-dimensional data briefly. The steps in creating a 2D plot is: \n",
    "1. Decide what the $x$-axis and $y$-axis represent in your graph (i.e. which features do you want to use to visualize a data point)\n",
    "2. Decide how the classification of each data point is represented (e.g. shape, color, size of markers)\n",
    "3. Create a scatter plot using ```matplotlib.pyplot``` by passing it the arrays for $x$-axis and $y$-axis and other parameters where fit. \n",
    "4. Call ```plt.show()``` at the end to render the graph\n",
    "\n",
    "### Creating a 3D plot with Matplolib\n",
    "\n",
    "To plot a 3D graph using ```matplotlib```, set the projection parameter for a plot or subplot to ```'3d'```. We will also need to decide the data that defines the $x$, $y$, $z$ axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the data using matplotlib.pyplot\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19536c63",
   "metadata": {},
   "source": [
    "### Spliting the dataset\n",
    "\n",
    "We want to split the data into a train and test subsets. There are 2 steps in spliting the data:\n",
    "1. Preprocess the data \n",
    "2. Split the data into random train and test subsets\n",
    "\n",
    "#### Preprocessing the Data\n",
    "\n",
    "Before we split the data, we want to make sure we can visualize the desicion boundry of the SVM later on. To do that, we need to reduce the higher-dimensional dataset to a 2D dataset. We can do this by either:\n",
    "1. dropping some features, or \n",
    "2. projecting the dataset down to 2D using Singlular Value Decomposition (SVD)\n",
    "\n",
    "If you choose the second route to avoid loosing information from the dataset, you can add this import statement to perform SVD: ```from sklearn.decomposition import PCA```\n",
    "\n",
    "#### Spliting the Dataset into Random Train and Test Subsets\n",
    "\n",
    "For the Iris dataset, the data is organized by their classifications. Irises that are classified as Setosa are enlisted in the set first, followed by irises that are classified as Versicolour, then Virginica. We want to make sure that both the train and test subsets are representative of the original dataset. So, instead of spliting the Iris dataset in its original order, it might make sense for us to create a way to create the train and test set by randomly retrieving unique entries from the original dataset. \n",
    "\n",
    "You are asked to implement a solution without the help of scikit-learn. Think of a way to split the dataset so that you end up with two subsets with non-overlapping entries which samples from the original dataset.\n",
    "\n",
    "At the end, we should have 4 sets of data objects:\n",
    "1. Training input data\n",
    "2. Training targets\n",
    "3. Testing input data\n",
    "4. Testing targets\n",
    "\n",
    "#### A note on the lack of Validation Set for our training\n",
    "Recall that in textbook 2.2.2 (p. 20), we learned about the 3 types of datasets for machine learnin purposes: training, testing, and validation set. You might be wondering why don't we need a validation set for this problem. This is because, for this lab, we will be training the model without fine-tuning its hyperparameters during the training process. For a more complex learning problem, though, we may consider also creating a validation set so that we may evaluate the model's performance during training, and make adjustments to its hyperparameters accordingly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Preprocess the data\n",
    "reduced_data = ???\n",
    "\n",
    "# TODO: Split the data into two sets\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a76ea6",
   "metadata": {},
   "source": [
    "### Set Up a Support Vector Machine & Training the Classifier\n",
    "\n",
    "In the beginning of this lab, we introduced the origin of the Iris dataset. We know that this dataset was used as an example of linear discriminant analysis. Using this fact, select a support vector classifier from ```sklearn.svm``` as our SVM for the classification of this dataset. \n",
    "\n",
    "After setting up one of scikit-learn's support vector machines, training is done with just one line of code. Your model should be trained before we move on to visualizing the decision boundries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pick a SVM from sklearn.svm\n",
    "from sklearn.svm import ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267aee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up the classifer and train it\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63773d",
   "metadata": {},
   "source": [
    "### Visualizing the Decision Boundries\n",
    "Sometimes, it might be helpful to visualize our classifer's behavior by visualization its decision boundries. Scikit-learn has a function that does exactly that, so we should take advantage of that. Plot decision boundries using ```DecisionBoundaryDisplay```. The required module is imported for you below. Please see the documentation for this function to determine how to plot it. You will need to use scikit-learn with matplotlib for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d051a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# TODO: Use both matplotlib.pyplot and sklearn.inspection.DecisionBoundaryDisplay \n",
    "#       to visualize the learned decision boundries \n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b0372",
   "metadata": {},
   "source": [
    "### Testing the Classifier\n",
    "\n",
    "To test the classifier, we feed the test set into the classifer and have it predict the classes of the test inputs. Additional to obtaining the classifier's prediction for the test set, it might also be interesting to look at the confusion matrix and the accuracy of the prediction to get a general sense on how it's performing. \n",
    "\n",
    "Unlike binary classification, there are no positive or negative classes in a confusion matrix for a 3-class classifier. \n",
    "\n",
    "The rows and columns are organized as below:\n",
    "\n",
    "\n",
    "\n",
    "|                   |Setosa (actual)|Versicolour (actual)|Virginica (actual)|\n",
    "|-------------------|---------------|--------------------|------------------|\n",
    "|Setosa (pred.)     | correct pred. |  incorrect pred.   |  incorrect pred. |\n",
    "|Versicolour (pred.)|incorrect pred.|    correct pred.   |  incorrect pred. |\n",
    "|Virginica (pred.)  |incorrect pred.|  incorrect pred.   |    correct pred. |\n",
    "\n",
    "Use the imported ```metrics``` module to see how well your classifier is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af16cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# TODO: Get the classifier's prediction on the test set\n",
    "???\n",
    "\n",
    "# TODO: Calculate the accuracy and the confusion matrix using sklearn.metrics\n",
    "print(\"Accuracy: \",???)\n",
    "print(\"Confusion Matrix:\\n\", ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726d6d5",
   "metadata": {},
   "source": [
    "## Bonus Task\n",
    "\n",
    "Try to improve the accuracy of the prediction by any mean. For example, you could try to use a different SVM model, or you might perform regularization as a preprocessing step in the pipline, or you might adjust the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b3eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get a better accuracy score by any mean!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
